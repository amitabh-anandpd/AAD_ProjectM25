# Dinicâ€™s Algorithm (Arun)
Implementation folder for Dinicâ€™s Algorithm.

Why I'm Focusing on Dinic's Algorithm

When I was diving into network flow, I wanted to find the fastest way to calculate the maximum flow in a graph. Dinic's algorithm is a game-changer.

It's an extremely fast, strongly polynomial algorithm. This is a big deal because it means its runtime doesn't depend on the (potentially massive) capacity values on the edges.

What's really remarkable is its performance on bipartite graphs. It can solve the unweighted bipartite matching problem in O(âˆšVâ‹…E) time, which is fast enough to handle ridiculously large graphs. If you're doing competitive programming, this is almost always the algorithm you'll want to use for max-flow.

It was revolutionary because it introduced several new concepts all at once:

Building a level graph.

Finding a blocking flow.

Combining different graph traversals (BFS and DFS) in a clever way.

â˜• The Main Idea: The "Coffee Shop" Analogy

Before diving into the technical steps, here's an analogy I like.

Imagine you're at a starting point (the source, S) and you want to get to a coffee shop (the sink, T). You don't know the exact path, but you know the coffee shop is generally east of you.

If you want to get there, would you start by walking south? Or northwest? Probably not. The only sensible directions are east, northeast, and southeast. You'd use a heuristic: only move in directions that make positive progress toward your goal.

This is the central idea of Dinic's. We don't want to waste time exploring paths that take us further away from the sink. We need a way to guide our search.

ðŸ“ˆ The Level Graph (Our Guiding Heuristic)

Dinic's algorithm creates this "guiding" heuristic by building a level graph.

Hereâ€™s how it works:

We run a Breadth-First Search (BFS) starting from the source (S) on the current residual graph.

The "level" of any node is its shortest path distance (in number of edges) from S. So, S is at level 0, its direct neighbors are at level 1, their neighbors are at level 2, and so on.

The level graph only includes edges (u, v) that go from a node u at level L to a node v at level L+1.

This is our "coffee shop" rule! It instantly prunes all useless edges:

Backwards edges (going from L+1 to L) are ignored.

Sideways edges (going from L to L) are ignored.

We only ever move "forward" toward the sink, guaranteeing we're making progress. We also, of course, only consider edges that have a remaining capacity greater than zero.

ðŸ› ï¸ The Algorithm: Steps and Blocking Flows

The algorithm works in phases. In each phase, we find a "blocking flow" and add it to our total. We repeat this until no more flow can be sent.

Here are the steps:

Step 1: Build the Level Graph

Run a BFS from the source (S) on the current residual graph to find the level of every node.

Step 2: Check if Sink is Reachable

After the BFS, if the sink (T) was not reached, it means there is no path left from S to T.

We're done! The algorithm terminates, and we return the total max flow we've found so far.

Step 3: Find a Blocking Flow

If the sink was reached, we now use our new level graph.

We find augmenting paths from S to T by running one or more Depth-First Searches (DFS).

Crucially: The DFS is only allowed to use edges in the level graph (i.e., edges from level L to L+1).

For each path we find:

Calculate its bottleneck capacity (the smallest remaining capacity on the path).

Add this bottleneck value to our total max flow.

Update the residual capacities along the path (decreasing forward capacity, increasing backward capacity).

Step 4: Repeat until Blocked

We keep running this DFS (Step 3) and pushing flow until we cannot find any more S-T paths in the current level graph.

This state is called a blocking flow. It means we've "saturated" the level graphâ€”at least one edge on every S-T path in that specific level graph is now full.

Step 5: Repeat the Whole Process

Once a blocking flow is reached, we discard the old level graph.

We go back to Step 1 and build a brand new level graph based on the current residual capacities.

We repeat this entire process (Build Level Graph -> Find Blocking Flow) until the BFS in Step 2 fails to reach the sink.

âš¡ A Critical Optimization: Pruning Dead Ends

There's one last trick that makes this algorithm incredibly fast.

During the DFS phase (Step 3), what happens if we explore a path that leads to a "dead end"? (A node from which we can't reach the sink, because all its forward edges are saturated).

It would be very inefficient to re-explore this same dead-end path multiple times during the same blocking flow phase.

The solution is dead-end pruning. As our DFS backtracks from a node u because it's a dead end, we can effectively "prune" u. We mark it (or use a pointer system) so we don't bother visiting it again during this blocking flow phase. This ensures we only explore each "bad" path once, which simplifies the algorithm and speeds it up dramatically.

ðŸ“œ Summary

So, to recap, Dinic's algorithm is powerful because it cleverly combines:

BFS to build a level graph, which ensures we only make progress toward the sink.

DFS (with dead-end pruning) to find a blocking flow by efficiently pushing as much flow as possible through that level graph.

It repeats this process in phases, rebuilding the level graph each time until the max flow is found.


## Features

- âœ… **Complete Dinic's Algorithm Implementation** - Level-based BFS + blocking flow DFS
- âœ… **Rich Visualizations** - Per-iteration graphs showing residual networks and augmenting paths
- âœ… **Level-Based Layout** - Nodes positioned by BFS level for intuitive understanding
- âœ… **Performance Metrics** - Detailed timing and iteration tracking
- âœ… **Minimum Cut Extraction** - Automatically computes min-cut after max-flow
- âœ… **Modular Architecture** - Clean separation of algorithm, visualization, and metrics

## Project Structure

```
dinics/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dinics.py          # Pure algorithm implementation
â”‚   â”œâ”€â”€ graph_loader.py    # Graph file loading
â”‚   â”œâ”€â”€ visualizer.py      # Visualization engine
â”‚   â”œâ”€â”€ metrics.py         # Performance metrics collection
â”‚   â”œâ”€â”€ runner.py          # Single graph runner
â”‚   â”œâ”€â”€ batch_run.py       # Batch experiment runner
â”‚   â””â”€â”€ analyze.py         # Analysis and summary table
â”œâ”€â”€ graphs/
â”‚   â”œâ”€â”€ sample1.txt         # 6-node sample graph
â”‚   â”œâ”€â”€ sample2.txt         # 8-node sample graph
â”‚   â”œâ”€â”€ sample3.txt         # 10-node sample graph
â”‚   â”œâ”€â”€ layered_1.txt       # Layered graph (6 nodes)
â”‚   â”œâ”€â”€ layered_2.txt       # Layered graph (8 nodes)
â”‚   â”œâ”€â”€ layered_3.txt       # Layered graph (10 nodes)
â”‚   â”œâ”€â”€ crosslinked_1.txt   # Crosslinked graph (7 nodes)
â”‚   â”œâ”€â”€ crosslinked_2.txt   # Crosslinked graph (8 nodes)
â”‚   â”œâ”€â”€ crosslinked_3.txt   # Crosslinked graph (10 nodes)
â”‚   â”œâ”€â”€ dense_1.txt         # Dense graph (6 nodes)
â”‚   â”œâ”€â”€ dense_2.txt         # Dense graph (8 nodes)
â”‚   â”œâ”€â”€ dense_3.txt         # Dense graph (10 nodes)
â”‚   â”œâ”€â”€ sparse_1.txt        # Sparse graph (6 nodes)
â”‚   â”œâ”€â”€ sparse_2.txt        # Sparse graph (8 nodes)
â”‚   â”œâ”€â”€ sparse_3.txt        # Sparse graph (10 nodes)
â”‚   â”œâ”€â”€ bidirectional_1.txt # Bidirectional graph (6 nodes)
â”‚   â”œâ”€â”€ bidirectional_2.txt # Bidirectional graph (8 nodes)
â”‚   â””â”€â”€ bidirectional_3.txt # Bidirectional graph (10 nodes)
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ performance.csv            # Summary metrics
â”‚   â”œâ”€â”€ performance_iterations.csv # Per-iteration details
â”‚   â”œâ”€â”€ summary_table.csv          # Formatted summary table
â”‚   â”œâ”€â”€ output.txt                 # Flow distribution
â”‚   â””â”€â”€ run.log                    # Execution log
â”œâ”€â”€ visuals/
â”‚   â”œâ”€â”€ sample1/
â”‚   â”‚   â”œâ”€â”€ initial_graph.png
â”‚   â”‚   â”œâ”€â”€ final_flow_graph.png
â”‚   â”‚   â”œâ”€â”€ iteration_1/
â”‚   â”‚   â”‚   â”œâ”€â”€ initial_residual.png
â”‚   â”‚   â”‚   â”œâ”€â”€ selected_augmented_path.png
â”‚   â”‚   â”‚   â””â”€â”€ final_residual.png
â”‚   â”‚   â””â”€â”€ iteration_log.csv
â”‚   â””â”€â”€ [similar folders for each graph]
â””â”€â”€ requirements.txt
```

## Installation

### Prerequisites
- Python 3.7 or higher
- pip package manager

### Setup

```bash
# Install dependencies using pip
pip install networkx matplotlib pandas numpy scipy

# Or use a virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

## Usage

### Single Graph Execution

Run Dinic's algorithm on a single graph file:

```bash
python3 code/runner.py --graph graphs/sample1.txt --source 0 --sink 5
```

**Options:**
- `--graph`: Path to graph file (e.g., `graphs/sample1.txt`)
- `--source`: Source vertex index (default: 0)
- `--sink`: Sink vertex index (required)
- `--family`: Graph family name (optional, auto-detected if not provided)

**Example:**
```bash
python3 code/runner.py --graph graphs/layered_1.txt --source 0 --sink 5
```

### Batch Experiments

Run Dinic's algorithm on all graphs in the `graphs/` directory:

```bash
python3 code/batch_run.py
```

This will:
- Auto-detect graph family from filename
- Process all `.txt` files in the `graphs/` directory
- Generate visualizations for each graph
- Collect performance metrics
- Save results to `results/performance.csv`

### Generate Analysis Table

After running experiments, generate a summary table:

```bash
python3 code/analyze.py
```

This creates:
- `results/summary_table.csv` - Summary table with all performance metrics
- Prints a formatted table to console

### Recommended Workflow

1. **Run experiments:**
   ```bash
   python3 code/batch_run.py
   ```

2. **Generate analysis:**
   ```bash
   python3 code/analyze.py
   ```

## Graph File Format

Graph files should follow this format:

```
<num_vertices> <num_edges>
<u> <v> <capacity>
<u> <v> <capacity>
...
```

**Example (`graphs/sample1.txt`):**
```
6 10
0 1 10
0 2 8
1 2 5
1 3 5
2 4 10
3 4 8
3 5 7
4 5 10
1 4 2
2 3 3
```

## Graph Families

The project includes the following graph families:

- **Sample Graphs** (`sample1.txt`, `sample2.txt`, `sample3.txt`) - Basic test cases
- **Layered Graphs** (`layered_1.txt`, `layered_2.txt`, `layered_3.txt`) - Clear level structure, great for Dinic's blocking flow
- **Crosslinked Graphs** (`crosslinked_1.txt`, `crosslinked_2.txt`, `crosslinked_3.txt`) - Layered with crossing edges to create extra paths
- **Dense Graphs** (`dense_1.txt`, `dense_2.txt`, `dense_3.txt`) - High connectivity, many edges
- **Sparse Graphs** (`sparse_1.txt`, `sparse_2.txt`, `sparse_3.txt`) - Tree-like structure, minimal edges
- **Bidirectional Graphs** (`bidirectional_1.txt`, `bidirectional_2.txt`, `bidirectional_3.txt`) - Forward and reverse edges for testing residual mechanics

## Visualization Features

### Color Coding
- **Green** - Source node
- **Purple** - Sink node
- **Light Blue** - Intermediate nodes
- **Cyan** - Augmenting path edges (highlighted)
- **Red** - Saturated edges (flow = capacity)
- **Royal Blue** - Flowing edges (flow > 0)
- **Gray** - Unused edges
- **Orange (dashed)** - Reverse residual edges

### Visual Elements
- **Edge Labels**: Format `flow/capacity` on each edge
- **Level Labels**: Each node displays its BFS level (L0, L1, L2...) or âˆž if unreachable
- **Curved Edges**: Prevents overlapping for better readability
- **High DPI**: 250 DPI for report-quality images
- **Residual Graphs**: Dashed lines for residual edges

## Performance Metrics

The algorithm tracks:
- `total_time` - Total algorithm runtime
- `bfs_time_total` - Cumulative BFS phase time
- `dfs_time_total` - Cumulative DFS traversal time
- `num_iterations` - Number of BFS phases
- `num_augmenting_paths` - Total augmenting paths found
- `max_flow` - Maximum flow value
- `min_cut_value` - Minimum cut value
- `min_cut_edges` - List of edges in the minimum cut

## Output Structure

### Results Directory (`results/`)
- `performance.csv` - Summary metrics for all graphs (family, graph_name, n, m, timings, flow, min_cut)
- `performance_iterations.csv` - Per-iteration details for each graph
- `summary_table.csv` - Formatted summary table (generated by analyze.py)
- `output.txt` - Flow distribution and min-cut for the last run
- `run.log` - Execution log

### Visuals Directory (`visuals/`)
For each graph, a folder is created with:
- `initial_graph.png` - Initial flow network
- `final_flow_graph.png` - Final flow network with max flow
- `iteration_1/`, `iteration_2/`, ... - Per-iteration visualizations:
  - `initial_residual.png` - Residual network before path
  - `selected_augmented_path.png` - Highlighted augmenting path
  - `final_residual.png` - Residual network after path
- `iteration_log.csv` - Detailed iteration metrics




